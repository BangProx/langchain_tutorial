{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to create a dynamic (self-constructing) chain\n",
    "- chain의 일부를 입력값에 따라 runtime에 할당하고 싶을때 사용하는 기법이다.\n",
    "- RunnableLambda의 속성을 활용해서 Dynamic chain을 구성 할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable, RunnablePassthrough, chain\n",
    "\n",
    "contextualize_instructions = \"\"\"\n",
    "Convert the latest user question into a standalone question given the chat history. \n",
    "Don't answer the question, return the question and nothing else (no descriptive text).\n",
    "\"\"\"\n",
    "\n",
    "contextualize_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_instructions),\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        (\"human\",\"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "contextualize_question = contextualize_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위에서 contextualize_instructions를 통해서 채팅 기록을 기반으로 새로운 질문을 생성하는 chain을 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_instructions = (\n",
    "    \"\"\"Answer the user question given the following context:\\n\\n{context}.\"\"\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [('system',qa_instructions),('human','{question}')]    \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chat Prompt Template을 구성한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chain\n",
    "def contextualize_if_needed(input_ : dict) -> Runnable:\n",
    "    if input_.get(\"chat_history\"):\n",
    "        #이 과정을 통해서 실제 출력값을 생성하는건 아니고 또 다른 Runnable을 반환한다.\n",
    "        return contextualize_question\n",
    "    else:\n",
    "        return RunnablePassthrough()\n",
    "\n",
    "@chain\n",
    "def fake_retriever(input_ : dict) -> str:\n",
    "    return \"egypt's population in 2024 is about 111 million\"\n",
    "\n",
    "full_chain = (\n",
    "    RunnablePassthrough.assign(qusetion = contextualize_if_needed).assign(\n",
    "        context=fake_retriever\n",
    "    )\n",
    "    | qa_prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만약에 `RunnablePassThrough`를 통해서 입력된 dict() 객체에 \"chat_history\"라는 key가 있다면 contextualize_question이라는 Runnable을 반환한다.\n",
    "    - 반환된 Runnable은 full chain이 실행되면 스스로 실행된다.\n",
    "- 만약에 없다면 `RunnablePassThrough`를 반환한다.\n",
    "\n",
    "여기서 `Runnable`이란 invoked, batched, streamed, transformed and composed 될 수 있는 일의 단위이다.\n",
    "- invoke/ainvoke: 하나의 input을 output으로 변경시켜주는 메서드\n",
    "- batch/abatch: 여러 개의 input을 output으로 변경시켜주는 메서드\n",
    "- stream/astream: 결과물을 stream 해준다.\n",
    "- astream_log: 출력 및 입력에서 선택된 중간 결과를 Stream 해준다.\n",
    "\n",
    "`Runnable`을 구성하는 주요 primitive(구성 요소)는 `RunnableSequence` and `RunnableParallel`이다.\n",
    "- `RunnableSequence`는 한 `Runnable`의 결과물을 다음 과정의 input으로 사용함으로써 `Runnable`들을 순차적으로 invoke한다.\n",
    "    - | 연산자를 사용하거나 Runnable의 list를 통해서 구성한다.\n",
    "- `RunnableParallel`은 동일한 input을 여러 `Runnable`들에 전달함으로써 `Runnable`들을 동시에 invoke한다.\n",
    "    - dict 타입을 활용해서 구성한다.\n",
    "\n",
    "출처 : [링크](https://api.python.langchain.com/en/latest/runnables/langchain_core.runnables.base.Runnable.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Egypt's population in 2024 is about 111 million.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke(\n",
    "    {\n",
    "        \"question\": \"what about egypt\",\n",
    "        \"chat_history\": [\n",
    "            (\"human\", \"what's the population of indonesia\"),\n",
    "            (\"ai\", \"about 276 million\"),\n",
    "        ],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "What\n",
      " is\n",
      " the\n",
      " population\n",
      " of\n",
      " Egypt\n",
      "?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in contextualize_if_needed.stream(\n",
    "    {\n",
    "        \"question\" : \"what about egypt\",\n",
    "        \"chat_history\": [\n",
    "            (\"human\", \"what's the population of indonesia\"),\n",
    "            (\"ai\", \"about 276 million\"),\n",
    "        ],\n",
    "    }\n",
    "):\n",
    "    print(chunk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
